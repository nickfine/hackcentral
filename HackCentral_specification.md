# HackDay Central - AI Maturity Accelerator (Project Spec for Cursor)

## Overview

HackDay Central is not primarily an event site.

It is a year-round **AI Maturity Accelerator** for the organisation - a participation hub that turns early adopter energy into reusable assets, workflows, and scaled capability. The annual Hack Day becomes an **output showcase**, not the sole purpose.

This system must remain valuable even if Hack Day stopped. The platform is measured by adoption, reuse, workflow transformation, and organisational maturity - not by event attendance or comms reach.

---

## Core product framing

### What HackDay Central is
A participation system that:

- converts early adopter experiments into reusable assets
- accelerates adoption through social pull and mentorship
- captures learning and failure patterns as institutional knowledge
- makes maturity visible, measurable, and motivating
- provides safe onboarding paths for late joiners
- institutionalises winning patterns via standardisation pathways

### What it is not
- not primarily an event microsite
- not primarily comms-only updates
- not an archive for random documents
- not a point-scoring system for “AI attempts”

---

## Operating principles (anti-theatre)

- **No points for trying AI - only points for reusable outputs and workflow change**
- Participation > perfection, but outputs must become reusable or they do not count as progress
- Library quality and trust comes before Library volume
- Late majority will copy before they create - design for reuse-first

---

## Success definition

If Hack Day stopped, the hub remains valuable because it continuously increases:

- **AI adoption (people)**
- **Asset reuse (Library)**
- **Workflow transformation (Projects)**
- **Capability maturity (org-level)**

Business anchor:
AI maturity is evidenced not by experimentation, but by measurable productivity improvement in high-frequency workflows (delivery, support, sales ops, internal enablement)

---

## Success metrics (explicit maturity curve bend)

### Primary targets
- Accelerate from **<20% to >50% regular AI-using contributors within 12 months**
- Early adopters seed **≥70% of reusable AI assets** (prompts, templates, agent blueprints, guardrails)

### Evidence of majority adoption
- increasing Library reuse rate
- increasing % projects shipping AI artefacts
- shrinking gap between early adopters and late joiners

### Metric definitions (anti-ambiguity)
- **Regular contributor** = contributed ≥1 AI artefact in last 30 days OR participated in ≥1 AI-supported project activity in last 30 days
- **AI artefact** = prompt template / agent blueprint / guardrail pattern / evaluation rubric / structured output used by others
- **Reuse** = referenced, copied, linked in submission, or attached to a project outcome
- **AI contribution in Library** = submitted a new AI artefact OR improved/verified an existing one

---

## Product modules

### 1) AI Maturity Dashboard (new top-level module)

Purpose:
- visualise org-wide maturity progress using a staged behavioural model
- make maturity visible as a shared scoreboard, not a leadership slogan

Maturity stages:
- **Experimenting → Repeating → Scaling → Transforming**

Dashboard views:
- Org scoreboard view (motivation + visibility)
- Team view (optional comparisons, with anonymity controls)

Signals to track:
- % employees with AI contributions in Library
- % projects using AI artefacts (prompts, guardrails, agents, summaries)
- weekly active AI contributors (via integrations where possible)
- % projects redesigning workflows around AI (maturity proxy)

---

### 2) People Module (enhanced AI stewardship + matchmaking)

The People module is a pull engine. It operationalises adoption by making support visible, enabling mentorship, and routing late joiners toward help.

#### Profile self-tags

Experience / comfort labels (psych safety + quality signalling):
- AI Newbie
- AI Curious
- AI Comfortable
- AI Power User
- AI Expert

Capability / contribution labels:
- AI Early Adopter
- Prompt Engineer
- Agent Builder
- Guardrails Specialist
- AI Mentor
- AI Reviewer (quality checking / verification)

Matchmaking behaviours:
- auto-highlight AI Helpers during project recruitment and idea support
- prioritise top Library contributors in “I can help with…” suggestions
- route “AI Newbie” profiles toward low-risk starter templates and mentor pairing

---

### 3) Mentor Matching Workflow (new feature)

Goal:
Convert early adopter energy into distributed enablement capacity without formal training programmes.

Flow:
- One-click “Get paired with an AI Mentor”
- 15-30 min pairing request
- calendar nudge / workflow support

Capacity protection:
- mentors opt into fixed monthly slot limits (e.g. 2-4 sessions/month)
- auto-close requests when mentor capacity is reached

---

### 4) Recognition + social proof mechanics

Goal:
Use visible recognition to create social proof, FOMO, and measurable value visibility.

Mechanics:
- badges / leaderboards for:
  - Most reused AI assets
  - Most verified AI assets
  - Fastest pull-through (early adopter joins → project completion)

Updates feed:
- AI Impact Stories:
  - “How Sarah’s prompt template saved 12 hours per week”

---

### 5) Library (compound engine for adoption)

The Library is not an archive. It is the compounding system that allows late joiners to copy proven patterns.

#### 5.1 AI Arsenal (new subsection)
A curated, high-trust section containing:
- curated prompts and templates
- agent blueprints
- reusable agent configurations
- failure analysis (“What hallucinated and why we fixed it”)
- guardrails and policy patterns

#### 5.2 Quality gates (prevents slop)
Asset statuses:
- Draft (default)
- Verified (reviewed or demonstrated reuse)
- Deprecated (superseded or unsafe)

Minimum metadata required:
- intended user / role
- context and when to use
- limitations
- risk notes (privacy, hallucination risk)
- example input
- example output

#### 5.3 Reuse visibility (compounding value)
Each Library item displays:
- referenced in X projects
- reuse counts
- contributor credit
- verified by (if applicable)

---

### 6) AI-assisted Library intelligence

Goal:
Make Library discoverable and context-aware - not a manual search problem.

Capabilities:
- AI search + auto-summarise:
  - “Show me best prompts for code review”
  - “Show me guardrails for confidentiality”

- recommendation engine when submitting ideas:
  - “Similar failed attempts used this guardrail”
  - “This prompt is widely reused in X projects”

---

### 7) Close/archive capture nudges (institutional learning loop)

On closing or archiving any idea/project, require or heavily nudge:

- AI Usage Summary
- AI tools/agents used
- time saved estimate
- failures and lessons

Outcome:
Normalises learning capture and accelerates institutional memory.

---

### 8) Late-majority barrier reduction

Goal:
Reduce anxiety, risk, and effort so that late majority can participate safely.

#### Ultra-low-friction onboarding paths
In Submit mode:
- AI Assist generates structured draft fields:
  - Problem
  - Who it helps
  - Why now

Programme templates:
- AI Experiment Starter
- Copilot prompt pack for your role

Default first behaviour:
- Start by reusing an AI Arsenal item (copy first, create later)

#### Graduated nudges by user state
- New users: Quick AI 101 micro-guide
- Inactive idea submitters: 2-min AI boost prompt linking to AI Arsenal
- Builders: show best guardrails and agent starter kits
- AI Newbies: route to low-risk templates + mentor pairing

#### Reduce perceived risk
- anonymous submission option
- sandbox mode for experiments
- no visibility until published by owner

---

### 9) Governance (2026 AI realities)

Avoid “stage gate” framing - use AI readiness checks.

Building stage requires:
- AI impact hypothesis (time saved, error reduction, throughput gain)
- lightweight risk check:
  - bias
  - data privacy
  - misuse

Incubation stage requires:
- sponsor commitment to agent ops review when agentic components exist

---

## Metrics suite (pull + maturity)

Leading indicators:
- % ideas receiving AI-related comment/support within 7 days
- % late joiners referencing Library items
- % mentor requests completed within 14 days

Execution:
- % projects graduating with AI artefacts
- agent reuse rate
- % projects with Verified assets attached

Distribution:
- early adopter concentration (avoid Gini > 0.7)
- frontline vs leader usage gap

Maturity proxies:
- weekly active AI contributors
- % projects redesigning workflows around AI

---

## Standardisation pathways (institutionalisation mechanics)

When a Library artefact exceeds a reuse threshold (e.g. reused >10 times), it graduates into:

- approved template pack (role-based)
- default workflow module
- internal playbook inclusion
- safe starter kit for late joiners

This converts early adopter innovation into standardised organisational capability.

---

## Roadmap

### Phase 1 - MVP (expanded)
Include:
- basic AI Assist in Submit
- seed Library with 20-30 high-quality AI prompts/templates from early adopters
- launch AI Arsenal in Library (curated, structured, high trust)

Aim:
instant vibrancy and credibility.

### Phase 1.5 - pull dynamics before Programmes engine
Include:
- People enhancements
- experience/comfort self labels + capability labels
- mentor matching workflow

Reason:
accelerates pull dynamics early, reduces reliance on top-down adoption.

### Phase 3 - programmes and agent templates
- agent and programme templates
- judge / demo processes that allow agent-assisted evaluation (where appropriate)

### Phase 4 - continuous optimisation
- AI admin nudges:
  - “This project hasn’t posted an AI lesson - want help summarising?”
- A/B tests on onboarding
- continuous optimisation of adoption loop mechanics

---

## Implementation notes for Cursor

This spec is intended to drive:

- PRDs and tickets per module
- API surface for People / Library / Projects / Metrics
- component planning for Dashboard, Arsenal views, profile tags
- event capture for maturity metrics and behavioural lifecycle nudges
- phased delivery aligned to MVP → pull engine → programme system

The system should prioritise:
- low friction participation
- high trust Library assets
- measurable reuse and workflow transformation
- visible maturity scoreboard
